# Credit-Card-Default-Prediction
INTRODUCTION:
Machine learning and artificial intelligence are popular tools in financial analysis, from portfolio management to risk predictions. Our group chose to use credit card default prediction as our first glimpse at the application of machine learning in finance.
In this project, we evaluated three machine learning algo- rithms: K-Nearest Neighbors, Logistic Regression, and Neu- ral Network, tested the impact of stratified cross-validation on the performance, and analyzed the contributions of fea- tures in predicting credit card default. We concluded that stratification has limited in improving the performance, and Neural Network outperformed other models. The findings and interpretation of our models can help financial institu- tions understand the default formation and predict it earlier.


DATA:
A. Exploratory Data Analysis
This data set has 30000 instances and 23 features. Data Types include 4 categorical data and 19 numerical data. First 5 features summarized the demographic information of bank clients, and the rest 18 features are the historical payment information. A brief description of each features showing in TABLE I below. Our goal is to predict credit card default by using these features. No missing value detected for this dataset, so handling missing will not be performed in data preprocessing. However, we discovered data imbalance for the predict variable which indicates healthy credit is a lot more than credit default.

B. Data Preprocessing and Feature Engineering
The ID variable in the dataset is solely used to identify rows and has no contribution to the default predictions. As a result, we removed this column for simplifying analysis and modeling. The default payment next month is our dependent variable. The rest of the 23 variables are independent. Since many of the features are numerical values with different units, we performed normalization to our data using Min- MaxScaler from sklearn.


RESULTS:
A. Cross-Validation
To deal with the potential problem of data imbalance, we used the stratified K-fold algorithm. The stratified sampling allows for preserving the percentage of samples for each class. By doing so, our models are able to capture more characteristics and become more generalized. So, we imple- mented the stratifiedKFold from sklearn for three models.

B. KNN
The first model is the KNN model, which is the sim- plest algorithm. This algorithm cannot learn anything in the training period, but it stores the training dataset and learns from it at the time of making real-time predictions. Since we couldn’t determine the relationship between features and target, KNN provides non-linear solutions. We chose the default number of neighbors 5. For the performance, we calculated the recall rate since it can calculate the fraction of customers who pay default that can be predicted as paying default. Its formula is shown in equation, Recall = TP/(TP+FN).
In this model, the recall rate is about 34.95%. This rate seems not really high since the number of customers that pay default is less than customers that do not pay. Fig.4 shows the confusion matrix, which is calculated by the sum of the 10 confusion matrices. The number we want to maximize is the true positive in the bottom-right quadrant, which represents the number of customers correctly detected as paying default from all customers that pay default. Now it is about 2319. We also computed the weighted F1 score instead of the accuracy, which would return the average considering the proportion for each label in the dataset. The reason why we did not use the accuracy is because the accuracy for each model is high whether the model fails to predict. Using the F1 score can better evaluate our model’s performance since it is using harmonic mean instead of simple mean. Equation for the F1 score is shown in equation, F1 = 2×precision×recall/(precision + recall).

C. Logistic Regression
The second model is the Logistic regression. We used the binary logistic regression because our target values’ type is binary. We used the solver lbfgs. Other solvers such as saga, liblinear, and newton-cg all failed to get higher F1 or recall rate because they usually recognize to accommodate large dataset or multi-class problem. lbfgs solver is recommended to use for small datasets since our dataset has only 30 thousands observations and 23 features. The downside of lbfgs solver is that it can only support L2 regularization, which punishes the loss function to get high beta coefficients. L2 is a better choice than L1 because of its vary benefits, such as optimizing the mean cost while L1 penalty only reduces the median explanation. L2 is able to deal with dataset that has small number of features while L1 is used when there are a high number of features. However, the F1 and recall rate are the lowest among all three models. The F1 is about 76.86% and the recall is 22.90%. 
From the ROC curve, we observed that the curve is located closer to the diagonal, which means it is not strongly accurate. And the true positive value of the confusion matrix is also the least. Overall, logistic regression seems not working well.

D. Neural Network
Our last model is neural network. We used multi-layer perceptron MLPClassifier for our model training. It is the best performance algorithm among all three models we used. Before any enhancement, the recall rate of neural network model is about 35.93%. The F1 score is 79.71%. And the true positive is the highest value compared to other models, which means it has the best performace in predicting default payment.

E. Model Enhancement - Neural Network
Model enhancement of neural networks done by analyzing different parameters for the model. We chose the solver to be lbfgs which outperforms Adam the default solver) when the dataset is relatively small. Learning rate controls how quickly the model is adapted to the problem, so it is important to pick a learning rate for the model.
Diagnostic plots can be used to investigate how the learning rate and batch size impact. Through many experiments with different parameters, we decided the learn- ing rate to be 0.001, and to match the small learning rate we have 20 as our batch size. Hidden layers are the collection of small neurons which transfer the data and the training layer to layers, it plays a vital role in neural networks. Failing to choose a proper hidden layer will result in over-fitting and under-fitting conditions.
After trying neurons ranging from 50 up 500 with both single and multi-layer, a neuron of 400 nodes each layer and single hidden layer has the best performance. The activation function is RELU because it performs better as compared to traditionally used activation functions such as Sigmoid and Hyperbolic-Tangent functions. It is fast and easy to calculate. We also applied StratifiedKFold with 10 splits here. After model enhancement, recall rate is 37.21% and F1 score is 79.93%. This model has about 1-2% better performance than other models.
